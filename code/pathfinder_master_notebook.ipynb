{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pathfinder Master Notebook  \n",
    "### (Author - Mark McDonald - 5/12/2019)\n",
    "This notebook uses the following support scripts to preprocess data and train models:  \n",
    "1) data_preprocess.py (imported as pf_preprocess)  \n",
    "2) video_support_processes.py (imported as pf_video)  \n",
    "3) models.py (imported as pf_models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 - Set Backend For GPU Support\n",
    "This can be commented out if an Nvidia GPU is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# fix libiomp5.dylib error for mac\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL SETTING\n",
    "# Here we override the keras backend env variable to use plaidml\n",
    "# plaidml can make use of AMD GPUs \n",
    "# This assignment needs to be added before loading keras libraries\n",
    "\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# to install plaidML, activate appropriate environment and then:\n",
    "#   pip install -U plaidml-keras\n",
    "#   plaidml-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 - Check that you are in the correct current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are in the 'code' directory\n",
      "/Users/markmcdonald/Desktop/pathfinder/code\n"
     ]
    }
   ],
   "source": [
    "# Check that the current directory is '.../code'\n",
    "pwd = !pwd\n",
    "if pwd[0][-4:] != \"code\":\n",
    "    print(\"ERROR: You're currently not in the project's code directory.\")\n",
    "    print(pwd[0])\n",
    "else:\n",
    "    print(\"SUCCESS: You are in the 'code' directory\")\n",
    "    print(pwd[0])\n",
    "    import data_preprocess as pf_preprocess\n",
    "    import models as pf_model\n",
    "    import video_support_processes as pf_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 - Set Variables\n",
    "These must be set to match the local environment.  \n",
    "No other items in the notebook need to be modified except these varables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is where data downloaded from http://www.mikeprocopio.com/labeledlagrdata.html should exist\n",
    "original_train_dir = '/Volumes/Photos-BACKUP/89FinalProject/89FinalData/orig_train_data'\n",
    "original_test_dir = '/Volumes/Photos-BACKUP/89FinalProject/89FinalData/orig_test_data'\n",
    "\n",
    "# This is where directories containing data extracted from original data will be placed.\n",
    "base_dir = '/Volumes/Photos-BACKUP/89FinalProject/89FinalData'\n",
    "\n",
    "# This is the percentage of validation data\n",
    "val_split = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Found original train data directory: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/orig_train_data\n",
      "SUCCESS: Found original test data directory: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/orig_test_data\n",
      "SUCCESS: Found base directory: /Volumes/Photos-BACKUP/89FinalProject/89FinalData\n"
     ]
    }
   ],
   "source": [
    "# Test that supplied source data directory exists\n",
    "if not os.path.exists(original_train_dir):\n",
    "    print(\"ERROR: Please supply a valid local directory that hold downloaded source train data.\")\n",
    "    print(\"Source data can be downloaded from http://www.mikeprocopio.com/labeledlagrdata.html.\")\n",
    "else:\n",
    "    print(\"SUCCESS: Found original train data directory: {}\".format(original_train_dir))\n",
    "\n",
    "# Test that supplied test data directory exists\n",
    "if not os.path.exists(original_test_dir):\n",
    "    print(\"ERROR: Please supply a valid local directory that hold downloaded source test data.\")\n",
    "    print(\"Source data can be downloaded from http://www.mikeprocopio.com/labeledlagrdata.html.\")\n",
    "else:\n",
    "    print(\"SUCCESS: Found original test data directory: {}\".format(original_test_dir))\n",
    "    \n",
    "\n",
    "# Test that supplied base data directory exists\n",
    "if not os.path.exists(base_dir):\n",
    "    print(\"ERROR: Please create a directory that will be used to store extracted source data.\")\n",
    "    print(\"ERROR: Doesn't Exist: \", base_dir)\n",
    "else:\n",
    "    print(\"SUCCESS: Found base directory: {}\".format(base_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Validation Directories\n",
    "Create necessary support directories under the 'base_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Directory already existed and was retained: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/train\n",
      "Train Images directory already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/train/images\n",
      "Train Images data directory already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/train/images/data\n",
      "Train Masks directory already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/train/masks\n",
      "Train Masks data directory already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/train/masks/data\n",
      "Video Dir already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/train/video\n",
      "Validation Directory already existed and was retained: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/val\n",
      "Validation Images Dir already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/val/images\n",
      "Validation Data Images Dir already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/val/images/data\n",
      "Validation Masks Dir already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/val/masks\n",
      "Validation Data Masks Dir already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/val/masks/data\n",
      "Validation Directory already existed and was retained: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/test\n",
      "Test Images Dir already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/test/images\n",
      "Test Data Images Dir already exists: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/test/images/data\n",
      "Results Directory already existed and was retained: /Volumes/Photos-BACKUP/89FinalProject/89FinalData/results\n"
     ]
    }
   ],
   "source": [
    "# Here we establish the supporting directories for our training and validation data\n",
    "# The True parameter will erase any pre-existing data in the directries\n",
    "# Setting this parameter to False will only set the variables and assumed that directories already exist with data\n",
    "train_dir, train_video_dir, val_dir, test_dir, results_dir = pf_preprocess.set_directories(base_dir, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 -  Extract Data From Downloaded Files\n",
    "Use the downloaded MAT files to extract the image and mask data saving the images and masks to supporting directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we extract the image and mask data from our original data stored in MAT files\n",
    "pf_preprocess.create_img_and_mask_data(original_train_dir, train_dir)\n",
    "\n",
    "# For the test directories, masks won't be created\n",
    "pf_preprocess.create_img_and_mask_data(original_test_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation images and masks are pulled out of the training set and put into validation directories\n",
    "pf_preprocess.create_val_set(train_dir, val_dir, val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have images and masks in separate directories and files.  \n",
    "These will be used to train our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIDEBAR: Understanding the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to understand the data that we are working with.  You can skip this, if you like, and proceed to step 5.  \n",
    "\n",
    "The downloaded data is in MAT file format.  The MAT files contain a dictionary or data elements, some of which we will use for our process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review the original data\n",
    "import scipy.io\n",
    "mat_file_path = os.path.join(original_train_dir, 'labeled_lagr_data_640x480_DS1A/frame001.mat')\n",
    "mat_file = scipy.io.loadmat(mat_file_path)\n",
    "# Dictionary entries in the MAT file\n",
    "for k in mat_file.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each MAT file has a number of pieces of data.  For this project, we will only use  \n",
    "the 'im_rgb' and 'manual_human_labeling_mask'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original images are 640 x 480 (note that this is a vertical format)\n",
    "orig_image = mat_file['im_rgb']\n",
    "print(\"Image Shape: \", orig_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Masks are the same size as the images but only one channel.\n",
    "orig_mask = mat_file['manual_human_labeling_mask']\n",
    "print(\"Mask Shape: \",orig_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The values in the mask range from 0 to 2 as expected\n",
    "import numpy as np\n",
    "print(np.amin(orig_mask))\n",
    "print(np.amax(orig_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0's are the ground plane.  1's are obstacles and 2's are unidentified.   \n",
    "We are only interesting in the ground plane.  \n",
    "We will modify these masks later so that they contain only 0's and 1's.  \n",
    "First, we are going to create supoerimposed videos of the image sequences  \n",
    "with the 3-value masks. This will make for a more interesting presented overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test file does not have the same data elements\n",
    "mat_test_file_path = os.path.join(original_test_dir, 'labeled_lagr_data_640x480_DS2A_SupplementalFrames_101-624/frame620.mat')\n",
    "mat_test_file = scipy.io.loadmat(mat_test_file_path)\n",
    "# Dictionary entries in the MAT file\n",
    "for k in mat_test_file.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the test files are missing the labeled mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the SIDEBAR\n",
    "Let's get back to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 - Create a superimpsed video file based on original data\n",
    "Use the images and masks to create a video with the masks superimposed on the images.  These videos are a simple demonstration to show what a superimposed mask looks like from the training data set.  These will be perfect masks because they were manually created.  Later, we will develop a model that will create the masks for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pf_video.create_superimposed_video_from_MATFiles(original_train_dir, train_video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a video that has an overlay of the path for each of the training scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6 - Change the mask data to Binary Values\n",
    "The mask data contains 3 values that represent path(0), obstacle(1) and unknown(2).  \n",
    "We are only interested in what is a path or not so using 3 values to train our model is not necessary  \n",
    "and could potentially cause confusion during training and slow the training process with no benefit.  \n",
    "We will change the masks to reflect the binary nature of our desired masks where the path is a 1 and everything else is a 0.  \n",
    "This change is done in-place and will change the original masks.  \n",
    "If we want 3-value masks again, we will need to recreate our dataset with the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In-place conversion of original masks to binary masks for training purposes\n",
    "# This will destory the original masks and repalce them with binary masks\n",
    "pf_preprocess.convert_jpg_mask_to_binary_mask(os.path.join(train_dir, 'masks'))\n",
    "pf_preprocess.convert_jpg_mask_to_binary_mask(os.path.join(val_dir, 'masks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Image and Overlay\n",
    "Take a sample to see that the conversion produces a reasonable mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displays and image with a mask overlay\n",
    "from keras.preprocessing import image as kimage\n",
    "def display_overlay(image, mask, ispath=True):\n",
    "    if ispath:   \n",
    "        image = kimage.load_img(image)\n",
    "        mask = kimage.load_img(mask)\n",
    "        image = np.uint8(image)\n",
    "        mask = np.asarray(mask)\n",
    "\n",
    "    noneType = type(None)\n",
    "        \n",
    "    if type(mask) != noneType:\n",
    "        # make red layer for mask\n",
    "        mask = np.moveaxis(mask, 2, 0)\n",
    "        new_mask = np.zeros((3, mask.shape[1], mask.shape[2]))\n",
    "        new_mask[0] = np.array(mask[0]==0)*180\n",
    "        new_mask[1] = np.array(mask[0]==1)*180\n",
    "\n",
    "        new_mask = np.moveaxis(new_mask, 0, 2)\n",
    "\n",
    "        image = np.uint8(new_mask*.3 + image*.7)\n",
    "            \n",
    "    if image.shape[2] != 3:\n",
    "        image = np.uint8((np.zeros(shape=(360,480,3))) + image)\n",
    "    plt.imshow(np.uint8(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imagefiles = sorted(os.listdir(os.path.join(train_dir, 'images', 'data')))\n",
    "maskfiles = sorted(os.listdir(os.path.join(train_dir, 'masks', 'data')))\n",
    "test_image = os.path.join(train_dir, 'images', 'data', imagefiles[0])\n",
    "test_mask = os.path.join(train_dir, 'masks', 'data', maskfiles[0])\n",
    "\n",
    "display_overlay(test_image, test_mask, ispath=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation is Complete\n",
    "We now have a local data set of images and masks separated by training and validation sets.\n",
    "We can now proceed to train our model.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7 -  Model Training\n",
    "Using get_models() will retrieve a series of defined models that can be trained.  \n",
    "get_models() will return a list of all defined models and a description of the model.  \n",
    "Each list entry consists of a tuple where the first element is a dictionary of paramters  \n",
    "and the second element is the corresponding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators (train, validate and test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Generators are defined in the models.py script.  \n",
    "One generator is defined for the training images and masks \n",
    "and another is defined for the validation images and masks.  \n",
    "Each generator will return an image and a mask for the image.  \n",
    "For the training generator, an augmented image and identically augmented mask is produced.\n",
    "Batch sizes are defined in the 'description' dictionary from the model_tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a model tuple to test the generators with\n",
    "model_tuples = []\n",
    "model_tuples.append(pf_model.get_autoenc_model2(epochs=40, \n",
    "                                                 rot=20, \n",
    "                                                 zoom=[.7,1.0], \n",
    "                                                 hflip=True, \n",
    "                                                 vflip=True,\n",
    "                                                 notes=\"test model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a support function to create a model.  The function will return a model as well as a 'description' dictionary that includes the settings used in the model.  This description is used during training to ensure that properly configured data generators are created and also serves as a way to see the model settings after it is done training and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator for training images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "description = model_tuples[0][0]\n",
    "train_generator = pf_model.get_img_mask_generator(os.path.join(train_dir, 'images'), \n",
    "                                             os.path.join(train_dir, 'masks'), \n",
    "                                             description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "image, mask = next(train_generator)\n",
    "\n",
    "image1 = np.uint8(np.asarray(image[0]*255)) # genereated image is scaled to 1/255\n",
    "mask1 = np.uint8(mask[0]) # mask wasn't rescaled because values are 0 or 1\n",
    "display_overlay(image1,mask1, ispath=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image produced is an augmented image where both the image and mask are identically augmented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator for validation images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_generator = pf_model.get_img_mask_generator(os.path.join(val_dir, 'images'), \n",
    "                                           os.path.join(val_dir, 'masks'), \n",
    "                                           description,\n",
    "                                           augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image, mask = next(val_generator)\n",
    "\n",
    "image = np.uint8(np.asarray(image[0]*255)) # genereated image is scaled to 1/255\n",
    "mask = np.uint8(mask[0]) # mask wasn't rescaled because values are 0 or 1\n",
    "display_overlay(image,None, ispath=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_overlay(image,mask, ispath=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A radomnly selected image and mask from the generator are overlayed.  \n",
    "We can see that the overlay matches the same augmentation as the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator for test images\n",
    "We will also use a generator to generate images from our test directory.  These images will be used to create masks from our model later.  This generator is created as a simplified way to generate test images in the same way that our model was trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = pf_video.get_test_img_generator(os.path.join(test_dir, 'images'), description, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = next(test_generator)\n",
    "display_overlay(image[0], None, ispath=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are certain that our data generators are doing what we want, we can start training a real model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, we will save the description and the trained model weights in a pickle variable\n",
    "# Here we define functions to help is with pickle file handling.\n",
    "import pickle\n",
    "\n",
    "# Use pickle to save history for later use\n",
    "def pickle_save(variable, save_path):\n",
    "    save_file = save_path + '/data.pickle'\n",
    "    if os.path.isdir(save_path) == 0:\n",
    "        os.mkdir(save_path)\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(variable, f)\n",
    "\n",
    "# Loading a prevously saved pickle file\n",
    "def pickle_load(pickel_filepath):\n",
    "    with open(pickel_filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to use our get_model() function to get a model with provided argument settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/dl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Get models for training\n",
    "model_tuples = []\n",
    "# The following model parameters were determined to provide the best overal results\n",
    "# model_tuples.append(pf_model.get_autoenc_model(epochs=40, \n",
    "#                                                  rot=30, \n",
    "#                                                  zoom=[.6,1.0], \n",
    "#                                                  hflip=True, \n",
    "#                                                  vflip=True,\n",
    "#                                                  dropout=.4,\n",
    "#                                                  notes=\"rot30-.6-1.0zoom_dropout=.4BEST\"))  \n",
    "model_tuples.append(pf_model.get_autoenc_model2(epochs=30, \n",
    "                                                 rot=30, \n",
    "                                                 zoom=[.6,1.0], \n",
    "                                                 hflip=True, \n",
    "                                                 vflip=True,\n",
    "                                                 dropout=.4,\n",
    "                                                 notes=\"rot30-.6-1.0zoom_dropout=.4-3072dim\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, is the training step.  This will train each model in the model_tuples list sequentially.  \n",
    "This step will automatically create a test video with the trained model.  \n",
    "Training progress charts can be seen in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating generator:\n",
      "\t/Volumes/Photos-BACKUP/89FinalProject/89FinalData/train/images\n",
      "\t/Volumes/Photos-BACKUP/89FinalProject/89FinalData/train/masks\n",
      "\tAugemntation:  True\n",
      "Found 480 images belonging to 1 classes.\n",
      "Found 480 images belonging to 1 classes.\n",
      "Creating generator:\n",
      "\t/Volumes/Photos-BACKUP/89FinalProject/89FinalData/val/images\n",
      "\t/Volumes/Photos-BACKUP/89FinalProject/89FinalData/val/masks\n",
      "\tAugemntation:  False\n",
      "Found 120 images belonging to 1 classes.\n",
      "Found 120 images belonging to 1 classes.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 360, 480, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 360, 480, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 180, 240, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 180, 240, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 180, 240, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 90, 120, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 90, 120, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 45, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 45, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 15, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 15, 20, 128)       147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 45, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 45, 60, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 90, 120, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 90, 120, 32)       18464     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 180, 240, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 180, 240, 16)      4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 360, 480, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 360, 480, 1)       145       \n",
      "=================================================================\n",
      "Total params: 342,049\n",
      "Trainable params: 342,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 258s 32s/step - loss: 2.3568 - val_loss: 0.8111\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 236s 30s/step - loss: 0.8899 - val_loss: 0.8693\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 313s 39s/step - loss: 0.6589 - val_loss: 0.8503\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 232s 29s/step - loss: 0.6590 - val_loss: 0.9251\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 238s 30s/step - loss: 0.6966 - val_loss: 1.1454\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 234s 29s/step - loss: 0.6642 - val_loss: 0.8488\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 237s 30s/step - loss: 0.7250 - val_loss: 0.8504\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 236s 29s/step - loss: 0.6298 - val_loss: 0.9587\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 234s 29s/step - loss: 0.6157 - val_loss: 0.8132\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 232s 29s/step - loss: 0.6528 - val_loss: 0.9061\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 234s 29s/step - loss: 0.6718 - val_loss: 0.8367\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 234s 29s/step - loss: 0.6089 - val_loss: 0.8852\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 232s 29s/step - loss: 0.7458 - val_loss: 0.8289\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 232s 29s/step - loss: 0.8055 - val_loss: 1.5996\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 234s 29s/step - loss: 0.8920 - val_loss: 0.9012\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 236s 29s/step - loss: 0.7102 - val_loss: 1.0998\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 233s 29s/step - loss: 0.6701 - val_loss: 0.8217\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 236s 30s/step - loss: 0.5940 - val_loss: 0.8967\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 235s 29s/step - loss: 0.6496 - val_loss: 0.8003\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 232s 29s/step - loss: 0.6022 - val_loss: 0.8584\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 232s 29s/step - loss: 0.6146 - val_loss: 0.8904\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 233s 29s/step - loss: 0.5993 - val_loss: 0.8086\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 233s 29s/step - loss: 0.6281 - val_loss: 0.9639\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 234s 29s/step - loss: 0.6083 - val_loss: 0.8507\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 231s 29s/step - loss: 0.6235 - val_loss: 0.7980\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 236s 29s/step - loss: 0.5642 - val_loss: 0.9574\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 233s 29s/step - loss: 0.5968 - val_loss: 0.9006\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 234s 29s/step - loss: 0.5760 - val_loss: 0.7650\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 231s 29s/step - loss: 0.6423 - val_loss: 0.8533\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 233s 29s/step - loss: 0.5873 - val_loss: 0.9055\n",
      "Final Accuracy:  0.9055\n",
      "{'Model_Type': 'CNN_encoder3072', 'input_height': 360, 'input_width': 480, 'color': True, 'output_height': 360, 'output_width': 480, 'l2_lambda': 0, 'final_activation': 'relu', 'optimizer': 'adadelta', 'loss': 'binary_crossentropy', 'learn_rate': None, 'batch_size': 64, 'epochs': 30, 'rotation': 30, 'zoom': [0.6, 1.0], 'hflip': True, 'vflip': True, 'dropout': 0.4, 'epoch_time': 1558478365, 'fill_mode': 'nearest', 'notes': 'rot30-.6-1.0zoom_dropout=.4-3072dim', 'name': '1558478365_CNN_encoder3072_epochs=30_notes=rot30-.6-1.0zoom_dropout=.4-3072dim', 'history': <keras.callbacks.History object at 0xb3aba34a8>, 'results': 0.905474861462911}\n",
      "Creating test image generator:\n",
      "\t/Volumes/Photos-BACKUP/89FinalProject/89FinalData/test/images\n",
      "Found 891 images belonging to 1 classes.\n",
      "Processing 891 images in 1 batches.\n",
      "Creating video name:  /Volumes/Photos-BACKUP/89FinalProject/89FinalData/results/1558478365_CNN_encoder3072_epochs=30_notes=rot30-.6-1.0zoom_dropout=.4-3072dim_loss=0.9055/1558478365_CNN_encoder3072_epochs=30_notes=rot30-.6-1.0zoom_dropout=.4-3072dim.mp4\n",
      "FPS:  20.0\n",
      "Processing batch: 1/1 | Max/Min/Avg mask value: 88.37999725341797/-0.0/15.289999961853027\n",
      "Video created /Volumes/Photos-BACKUP/89FinalProject/89FinalData/results/1558478365_CNN_encoder3072_epochs=30_notes=rot30-.6-1.0zoom_dropout=.4-3072dim_loss=0.9055/1558478365_CNN_encoder3072_epochs=30_notes=rot30-.6-1.0zoom_dropout=.4-3072dim.mp4\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "num_models = len(model_tuples)\n",
    "\n",
    "# a model_tupled object can contain multiple models\n",
    "# us pf_model.get_autoencoder_model() method to create tuples\n",
    "for i, model_tuple in enumerate(model_tuples):\n",
    "    description = model_tuple[0]\n",
    "    model = model_tuple[1]\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = description['optimizer']\n",
    "    loss = description['loss']\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    # Add Tensorboard capability\n",
    "    name = description[\"name\"]\n",
    "    logdir = os.path.join(base_dir, \"logs\" , name)\n",
    "    tensorboard = TensorBoard(log_dir=logdir)\n",
    "\n",
    "    # remove the logging directory for the model if it exists\n",
    "    if os.path.isdir(logdir):  shutil.rmtree(logdir)\n",
    "    \n",
    "    # get number of images to train and validate\n",
    "    num_timages = len(os.listdir(os.path.join(train_dir, 'images', 'data')))\n",
    "    num_vimages = len(os.listdir(os.path.join(val_dir, 'images', 'data')))\n",
    "    \n",
    "    # create generators based on the model description variables\n",
    "    train_gen = pf_model.get_img_mask_generator(os.path.join(train_dir, 'images'), \n",
    "                                             os.path.join(train_dir, 'masks'), \n",
    "                                             description,\n",
    "                                             augment=True)\n",
    "    val_gen = pf_model.get_img_mask_generator(os.path.join(val_dir, 'images'), \n",
    "                                         os.path.join(val_dir, 'masks'), \n",
    "                                         description,\n",
    "                                         augment=False)\n",
    "    \n",
    "    epochs = description[\"epochs\"]\n",
    "    batch_size = description[\"batch_size\"]\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # step_per_epcoch and val_steps is numbner of images / batchsize\n",
    "    history = model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=math.ceil(num_timages/batch_size), \n",
    "                                  epochs=epochs, verbose=1, \n",
    "                                  callbacks=[tensorboard],\n",
    "                                  validation_data=val_gen, \n",
    "                                  validation_steps=math.ceil(num_vimages/batch_size),\n",
    "                                  use_multiprocessing=True, \n",
    "                                  shuffle=True)\n",
    "        \n",
    "    results = model.evaluate_generator(val_gen, steps=math.ceil(num_vimages/batch_size))\n",
    "    \n",
    "    description.update({\"history\": history})\n",
    "    description.update({\"results\": results})\n",
    "    \n",
    "    acc = str(np.round(results, decimals=4))\n",
    "    print(\"Final Accuracy: \", acc)\n",
    "    \n",
    "    # Create overlay text and prepare a directory to save video and pickle file\n",
    "    overlay_text = name + \"_loss=\" + acc\n",
    "    source_dir = test_dir\n",
    "    target_dir = os.path.join(results_dir, overlay_text) \n",
    "    os.mkdir(target_dir)\n",
    "    \n",
    "    # Save description and model in a pickle file\n",
    "    pickle_save((description, model), target_dir)\n",
    "    \n",
    "    print(description)\n",
    "\n",
    "    # Create video with mask overlay generated from model\n",
    "    pf_video.create_video_with_test_data(source_dir, model, description, target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "A superimposed test video has been created in the directory indicated at the end of the output in the previous step.  The model parameters can be changed to explore settings that may provide better results.  Please let me know if you find a better combination of parameters.  mcdomx@me.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT PROCESSES: Variational AutoEncoder Design\n",
    "These steps are to help with model design so that the dimensions can be seen.  The final model is stored in the models.py support script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "input_img = Input(shape=(360, 480, 3))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Dropout(.1)(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = MaxPooling2D((3, 3), padding='same')(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((3, 3), padding='same')(x)\n",
    "\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# at this point the representation is (15, 20, 64) i.e. 19200-dimensional\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((3, 3))(x)\n",
    "\n",
    "# x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = UpSampling2D((3, 3))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models as pf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = pf_model.get_autoenc_model2(epochs=40, \n",
    "                                                 rot=30, \n",
    "                                                 zoom=[.6,1.0], \n",
    "                                                 hflip=True, \n",
    "                                                 vflip=True,\n",
    "                                                 dropout=.4,\n",
    "                                                 notes=\"rot30-.6-1.0zoom_dropout=.4BEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model[1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
